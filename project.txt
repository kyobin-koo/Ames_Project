# jisuhan



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import statsmodels.formula.api as smf
import seaborn as sns
from statsmodels.api import OLS
from statsmodels.formula.api import ols
import statsmodels.api as sm




df = pd.read_csv("./plotly_data/ames.csv")
# ì‚¬ìš©ìê°€ ì œì‹œí•œ ë³€ìˆ˜ ëª©ë¡ ì •ë¦¬
target_columns = [
    # í’ˆì§ˆ/ìƒíƒœ ê´€ë ¨
    'OverallQual', 'OverallCond', 'RoofStyle', 'ExterQual', 'ExterCond',
    'Exterior1st', 'HeatingQC', 'GarageCond', 'BsmtCond', 'BsmtQual',
    'KitchenQual', 'GarageQual', 'Foundation', 'PavedDrive',
    
    # ì—°ë„ ê´€ë ¨
    'YearBuilt', 'YearRemodAdd', 'GarageYrBlt',
    
    # ìš•ì‹¤ ê´€ë ¨
    'FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath',
    
    # ë©´ì  ê´€ë ¨
    'GrLivArea', 'TotalBsmtSF', 'GarageArea', 'WoodDeckSF',
    
    # ìœ„ì¹˜ ê´€ë ¨
    'TotRmsAbvGrd', 'Latitude', 'Longitude', 'SalePrice'
]

# ê²°ì¸¡ì¹˜ ê°œìˆ˜ í™•ì¸
missing_info = df[target_columns].isnull().sum()
missing_info = missing_info[missing_info > 0]
missing_info
df = df.dropna(subset=['Latitude', 'Longitude'])



# 4. ê²°ì¸¡ì¹˜ ì²˜ë¦¬ 
# 4-1. ì¹´í…Œê³ ë¦¬í˜• ê²°ì¸¡ì¹˜: ì§€í•˜ì‹¤/ì°¨ê³ ê°€ ì—†ëŠ” ê²½ìš° "None" ì²˜ë¦¬
df['GarageCond'] = df['GarageCond'].fillna("None")
df['BsmtCond'] = df['BsmtCond'].fillna("None")
df['BsmtQual'] = df['BsmtQual'].fillna("None")
df['GarageQual'] = df['BsmtQual'].fillna("None")

# 4-2. ìˆ˜ì¹˜í˜• ê²°ì¸¡ì¹˜: ì°¨ê³  ê±´ì¶•ì—°ë„ â†’ ì°¨ê³  ì—†ìŒì´ë©´ 0ìœ¼ë¡œ
df['GarageYrBlt'] = df['GarageYrBlt'].fillna(0)

df['BsmtFullBath'] = df['BsmtFullBath'].fillna(0)
df['BsmtHalfBath'] = df['BsmtHalfBath'].fillna(0)
df['TotalBsmtSF'] = df['TotalBsmtSF'].fillna(0)
df['GarageArea'] = df['GarageArea'].fillna(0)
df = df.drop(columns=['PID'])


df = df[target_columns]

# 5. ë²”ì£¼í˜• ë³€ìˆ˜ ì²˜ë¦¬í•˜ê¸°
# 5ï¸-1. ë²”ì£¼í˜• ë³€ìˆ˜ â†’ ìˆ˜ì¹˜í˜• ë³€í™˜ (Ordinal Encoding)
# ë²”ì£¼í˜• ë³€ìˆ˜ ìˆ˜ì¹˜í™” (Ordinal Encoding)
ordinal_mappings = {
    'ExterQual': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
    'ExterCond': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
    'HeatingQC': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
    'GarageCond': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
    'BsmtCond': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
    'BsmtQual': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
    'KitchenQual': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
    'GarageQual': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
}

# ë§¤í•‘ ì ìš©
for col, mapping in ordinal_mappings.items():
    df[col] = df[col].map(mapping)

df = pd.get_dummies(df, columns=['RoofStyle', 'Exterior1st', 'Foundation', 'PavedDrive'], drop_first=True)


# 6ï¸. ê²°ì¸¡ì¹˜ í™•ì¸
print(df.isnull().sum())


# LassoCV (ë³€ìˆ˜ 39ê°œ)
import pandas as pd
from sklearn.linear_model import LassoCV
from sklearn.preprocessing import StandardScaler

# ë…ë¦½ë³€ìˆ˜(X), ì¢…ì†ë³€ìˆ˜(y) ë¶„ë¦¬
X = df.drop(columns=['SalePrice'])
y = df['SalePrice']

# 1. ìŠ¤ì¼€ì¼ë§ (í‘œì¤€í™”)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)  # Xë¥¼ í‰ê·  0, í‘œì¤€í¸ì°¨ 1ë¡œ ë³€í™˜

# 2ï¸. LassoCV í•™ìŠµ (êµì°¨ê²€ì¦ìœ¼ë¡œ alpha ì°¾ê¸°)
lasso = LassoCV(cv=5, random_state=2025)
lasso.fit(X_scaled, y)

# 3. ê²°ê³¼ í™•ì¸
best_alpha = lasso.alpha_
coefficients = lasso.coef_

# 4. ê³„ìˆ˜ì™€ ë³€ìˆ˜ëª…ì„ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ì •ë¦¬
coef_df = pd.DataFrame({
    'Feature': X.columns,
    'Coefficient': coefficients
}).sort_values(by='Coefficient', key=abs, ascending=False)

print(f"Best alpha: {best_alpha}")
print(coef_df)



# Lasso alpha
from sklearn.linear_model import Lasso
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# ê¸°ì¤€ alpha (LassoCVì—ì„œ ë‚˜ì˜¨ ê°’)
base_alpha = lasso.alpha_

# ì‹¤í—˜í•  alpha ë°°ìˆ˜ë“¤
alpha_multipliers = [0.5, 1, 2, 5, 10, 15, 20, 25]
results = []

# ë°˜ë³µí•´ì„œ ëª¨ë¸ ëŒë¦¬ê¸°
for m in alpha_multipliers:
    alpha = base_alpha * m
    model = Lasso(alpha=alpha, max_iter=10000)
    model.fit(X_scaled, y)
    coef = model.coef_
    num_nonzero = np.sum(coef != 0)
    y_pred = model.predict(X_scaled)
    mse = mean_squared_error(y, y_pred)

    results.append({
        'alpha': round(alpha, 5),
        'multiplier': m,
        'num_features': num_nonzero,
        'mse': round(mse, 2)
    })

    # ë‚¨ì•„ ìˆëŠ” ë³€ìˆ˜ ì¶œë ¥
    print(f"\n[alpha = {alpha:.5f}]")
    print(f"ë‚¨ì€ ë³€ìˆ˜ ê°œìˆ˜: {num_nonzero}")
    print("ì„ íƒëœ ë³€ìˆ˜:")
    print(pd.Series(X.columns[coef != 0]).values)

# ê²°ê³¼ DataFrameìœ¼ë¡œ ì •ë¦¬
results_df = pd.DataFrame(results)
print("\nğŸ“Š Alphaë³„ ê²°ê³¼:")
print(results_df)

# ê·¸ë˜í”„ë¡œ ë³´ê¸°
plt.figure(figsize=(10, 5))
plt.plot(results_df['alpha'], results_df['num_features'], marker='o', label='Selected Features')
plt.xlabel('Alpha')
plt.ylabel('Number of Selected Features')
plt.title('Alpha ê°’ì— ë”°ë¥¸ ë³€ìˆ˜ ì„ íƒ ê°œìˆ˜ ë³€í™”')
plt.grid(True)
plt.legend()
plt.show()

from sklearn.model_selection import GridSearchCV, KFold

# alphaë¡œ ì°¾ê¸°
selected_features = ['OverallQual', 'ExterQual', 'KitchenQual' ,'YearBuilt', 'BsmtFullBath',
'GrLivArea', 'TotalBsmtSF' ,'GarageArea' ,'WoodDeckSF','RoofStyle_Hip' ]




# ì ˆëŒ€ê°’ìœ¼ë¡œ 10ê°œ ì°¾ê¸°
# 1. ì¢…ì†ë³€ìˆ˜ ë¡œê·¸ ë³€í™˜
y = (df['SalePrice'])  # log(1 + y)

# 2. ìƒìœ„ 10ê°œ í”¼ì²˜ ì„ íƒ (ê¸°ì¡´ coef_df ê¸°ì¤€)
top_features = coef_df[coef_df["Coefficient"] != 0]['Feature'].head(10).tolist()
X = df[top_features]

# 3. ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 4. Lasso + GridSearchCV ì„¤ì •
lasso = Lasso(max_iter=10000)
alphas = np.linspace(0.01, 1, 100)
cv = KFold(n_splits=5, shuffle=True, random_state=2025)
grid = GridSearchCV(estimator=lasso, param_grid={'alpha': alphas},
                    cv=cv, scoring='neg_mean_squared_error')
grid.fit(X_scaled, y)

# 5. ìµœì  alpha ë° ì„±ëŠ¥ í™•ì¸
print("Best alpha:", grid.best_params_['alpha'])
print("Best CV Score (MSE):", -grid.best_score_)

# 6. íšŒê·€ ê³„ìˆ˜ ì¶œë ¥
coef_df_top = pd.DataFrame({
    'Feature': top_features,
    'Coefficient': grid.best_estimator_.coef_}).sort_values(by='Coefficient', key=abs, ascending=False)

print(coef_df_top)




# ëª¨ë¸ ë¹„êµ
# model 1 ì•ŒíŒŒê°’
from statsmodels.formula.api import ols
model1 = ols(
    'SalePrice ~ OverallQual + ExterQual + KitchenQual + YearBuilt + BsmtFullBath + GrLivArea + TotalBsmtSF + GarageArea + WoodDeckSF + RoofStyle_Hip',
    data=df
).fit()

model1.summary()


# model 2 ì ˆëŒ€ê°’
model2 = ols(
    formula='SalePrice ~ GrLivArea + OverallQual + TotalBsmtSF + GarageArea + YearBuilt + OverallCond + BsmtFullBath + ExterQual + KitchenQual + BsmtQual',
    data=df
).fit()

model2.summary()


import numpy as np
import pandas as pd
from sklearn.linear_model import Lasso
from sklearn.model_selection import GridSearchCV, KFold
from sklearn.preprocessing import StandardScaler
import statsmodels.api as sm




df

# ìµœì¢… ì„ íƒ 10ê°œ ë³€ìˆ˜
top_features 
df[top_features]



import matplotlib.pyplot as plt
import seaborn as sns



# ì‹œê°í™”
plt.figure(figsize=(16, 12))
for i, col in enumerate(top_features):
    plt.subplot(4, 3, i + 1)
    sns.scatterplot(data=df, x=col, y='SalePrice', s=20)
    plt.xlabel(col)
    plt.ylabel('')

plt.tight_layout()
plt.show()

# ì´ìƒì¹˜ ì¡°ê±´
condition = (
    (df['GrLivArea'] <= 3500) &
    (df['GarageArea'] <= 1200) &
    (df['TotalBsmtSF'] <= 2500)
)

# ì´ìƒì¹˜ ì œê±°
df_clean = df[condition].copy()

import pandas as pd
from sklearn.preprocessing import MinMaxScaler


# ìŠ¤ì¼€ì¼ë§í•  ë³€ìˆ˜ ëª©ë¡
scale_columns = [
    'GrLivArea', 'TotalBsmtSF', 'GarageArea',   # ë©´ì 
    'OverallQual', 'KitchenQual', 'BsmtQual', 'ExterQual',  # í’ˆì§ˆ
    'OverallCond', 'BsmtFullBath',  # ìƒíƒœ
    'YearBuilt'  # ì—°ì‹
]

# MinMaxScaler ì ìš© (0~1 ë²”ìœ„)
scaler = MinMaxScaler()
df_clean[scale_columns] = scaler.fit_transform(df_clean[scale_columns])
df_clean = df_clean[scale_columns]


# ë©´ì  ê·¸ëŒ€ë¡œ
area_score = df_clean['GrLivArea'] + df_clean['TotalBsmtSF'] + df_clean['GarageArea']

# í’ˆì§ˆ/ìƒíƒœ/ì—°ì‹ ë’¤ì§‘ê¸° (1 - ê°’)
quality_score = (1 - df_clean['OverallQual']) + (1 - df_clean['KitchenQual']) + (1 - df_clean['BsmtQual']) + (1 - df_clean['ExterQual'])
condition_score = (1 - df_clean['OverallCond']) + (1 - df_clean['BsmtFullBath'])
year_score = 1 - df_clean['YearBuilt']  # ì—°ì‹ë„ ë°˜ì „

# ê°€ì¤‘ì¹˜ ê³±í•˜ê³  í•©ì‚°
df['MaintenanceScore'] = (
    area_score * 0.2 +
    quality_score * 0.3 +
    condition_score * 0.2 +
    year_score * 0.3
)




df['Latitude']










# import statsmodels.api as sm

# def forward_stepwise(X, y, threshold_in=0.05, verbose=True):
#     initial_features = []
#     remaining_features = list(X.columns)
#     selected_features = []

#     while remaining_features:
#         aic_with_candidates = []
#         for candidate in remaining_features:
#             try:
#                 model = sm.OLS(y, sm.add_constant(X[initial_features + [candidate]])).fit()
#                 aic_with_candidates.append((model.aic, candidate))
#             except:
#                 continue
        
#         if not aic_with_candidates:
#             if verbose:
#                 print("âš ï¸ ë‚¨ì€ ë³€ìˆ˜ë¡œ ë” ì´ìƒ ê°œì„  ë¶ˆê°€. ì¢…ë£Œí•©ë‹ˆë‹¤.")
#             break

#         aic_with_candidates.sort()
#         best_aic, best_candidate = aic_with_candidates[0]

#         if verbose:
#             print(f"Try adding {best_candidate:>20} | AIC: {best_aic:.2f}")

#         initial_features.append(best_candidate)
#         remaining_features.remove(best_candidate)
#         selected_features = initial_features[:]

#     return selected_features




# X = df.drop(columns=['SalePrice'])
# y = df['SalePrice']

# selected = forward_stepwise(X, y)
# print("\nâœ… ìµœì¢… ì„ íƒëœ ë³€ìˆ˜ ëª©ë¡:")
# print(selected)
