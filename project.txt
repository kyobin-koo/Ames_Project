# jisuhan



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import statsmodels.formula.api as smf
import seaborn as sns
from statsmodels.api import OLS
from statsmodels.formula.api import ols
import statsmodels.api as sm




df = pd.read_csv("./plotly_data/ames.csv")
# 사용자가 제시한 변수 목록 정리
target_columns = [
    # 품질/상태 관련
    'OverallQual', 'OverallCond', 'RoofStyle', 'ExterQual', 'ExterCond',
    'Exterior1st', 'HeatingQC', 'GarageCond', 'BsmtCond', 'BsmtQual',
    'KitchenQual', 'GarageQual', 'Foundation', 'PavedDrive',
    
    # 연도 관련
    'YearBuilt', 'YearRemodAdd', 'GarageYrBlt',
    
    # 욕실 관련
    'FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath',
    
    # 면적 관련
    'GrLivArea', 'TotalBsmtSF', 'GarageArea', 'WoodDeckSF',
    
    # 위치 관련
    'TotRmsAbvGrd', 'Latitude', 'Longitude', 'SalePrice'
]

# 결측치 개수 확인
missing_info = df[target_columns].isnull().sum()
missing_info = missing_info[missing_info > 0]
missing_info
df = df.dropna(subset=['Latitude', 'Longitude'])



# 4. 결측치 처리 
# 4-1. 카테고리형 결측치: 지하실/차고가 없는 경우 "None" 처리
df['GarageCond'] = df['GarageCond'].fillna("None")
df['BsmtCond'] = df['BsmtCond'].fillna("None")
df['BsmtQual'] = df['BsmtQual'].fillna("None")
df['GarageQual'] = df['BsmtQual'].fillna("None")

# 4-2. 수치형 결측치: 차고 건축연도 → 차고 없음이면 0으로
df['GarageYrBlt'] = df['GarageYrBlt'].fillna(0)

df['BsmtFullBath'] = df['BsmtFullBath'].fillna(0)
df['BsmtHalfBath'] = df['BsmtHalfBath'].fillna(0)
df['TotalBsmtSF'] = df['TotalBsmtSF'].fillna(0)
df['GarageArea'] = df['GarageArea'].fillna(0)
df = df.drop(columns=['PID'])


df = df[target_columns]

# 5. 범주형 변수 처리하기
# 5️-1. 범주형 변수 → 수치형 변환 (Ordinal Encoding)
# 범주형 변수 수치화 (Ordinal Encoding)
ordinal_mappings = {
    'ExterQual': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
    'ExterCond': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
    'HeatingQC': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
    'GarageCond': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
    'BsmtCond': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
    'BsmtQual': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
    'KitchenQual': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
    'GarageQual': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
}

# 매핑 적용
for col, mapping in ordinal_mappings.items():
    df[col] = df[col].map(mapping)

df = pd.get_dummies(df, columns=['RoofStyle', 'Exterior1st', 'Foundation', 'PavedDrive'], drop_first=True)


# 6️. 결측치 확인
print(df.isnull().sum())


# LassoCV (변수 39개)
import pandas as pd
from sklearn.linear_model import LassoCV
from sklearn.preprocessing import StandardScaler

# 독립변수(X), 종속변수(y) 분리
X = df.drop(columns=['SalePrice'])
y = df['SalePrice']

# 1. 스케일링 (표준화)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)  # X를 평균 0, 표준편차 1로 변환

# 2️. LassoCV 학습 (교차검증으로 alpha 찾기)
lasso = LassoCV(cv=5, random_state=2025)
lasso.fit(X_scaled, y)

# 3. 결과 확인
best_alpha = lasso.alpha_
coefficients = lasso.coef_

# 4. 계수와 변수명을 데이터프레임으로 정리
coef_df = pd.DataFrame({
    'Feature': X.columns,
    'Coefficient': coefficients
}).sort_values(by='Coefficient', key=abs, ascending=False)

print(f"Best alpha: {best_alpha}")
print(coef_df)



# Lasso alpha
from sklearn.linear_model import Lasso
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# 기준 alpha (LassoCV에서 나온 값)
base_alpha = lasso.alpha_

# 실험할 alpha 배수들
alpha_multipliers = [0.5, 1, 2, 5, 10, 15, 20, 25]
results = []

# 반복해서 모델 돌리기
for m in alpha_multipliers:
    alpha = base_alpha * m
    model = Lasso(alpha=alpha, max_iter=10000)
    model.fit(X_scaled, y)
    coef = model.coef_
    num_nonzero = np.sum(coef != 0)
    y_pred = model.predict(X_scaled)
    mse = mean_squared_error(y, y_pred)

    results.append({
        'alpha': round(alpha, 5),
        'multiplier': m,
        'num_features': num_nonzero,
        'mse': round(mse, 2)
    })

    # 남아 있는 변수 출력
    print(f"\n[alpha = {alpha:.5f}]")
    print(f"남은 변수 개수: {num_nonzero}")
    print("선택된 변수:")
    print(pd.Series(X.columns[coef != 0]).values)

# 결과 DataFrame으로 정리
results_df = pd.DataFrame(results)
print("\n📊 Alpha별 결과:")
print(results_df)

# 그래프로 보기
plt.figure(figsize=(10, 5))
plt.plot(results_df['alpha'], results_df['num_features'], marker='o', label='Selected Features')
plt.xlabel('Alpha')
plt.ylabel('Number of Selected Features')
plt.title('Alpha 값에 따른 변수 선택 개수 변화')
plt.grid(True)
plt.legend()
plt.show()

from sklearn.model_selection import GridSearchCV, KFold

# alpha로 찾기
selected_features = ['OverallQual', 'ExterQual', 'KitchenQual' ,'YearBuilt', 'BsmtFullBath',
'GrLivArea', 'TotalBsmtSF' ,'GarageArea' ,'WoodDeckSF','RoofStyle_Hip' ]




# 절대값으로 10개 찾기
# 1. 종속변수 로그 변환
y = (df['SalePrice'])  # log(1 + y)

# 2. 상위 10개 피처 선택 (기존 coef_df 기준)
top_features = coef_df[coef_df["Coefficient"] != 0]['Feature'].head(10).tolist()
X = df[top_features]

# 3. 스케일링
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 4. Lasso + GridSearchCV 설정
lasso = Lasso(max_iter=10000)
alphas = np.linspace(0.01, 1, 100)
cv = KFold(n_splits=5, shuffle=True, random_state=2025)
grid = GridSearchCV(estimator=lasso, param_grid={'alpha': alphas},
                    cv=cv, scoring='neg_mean_squared_error')
grid.fit(X_scaled, y)

# 5. 최적 alpha 및 성능 확인
print("Best alpha:", grid.best_params_['alpha'])
print("Best CV Score (MSE):", -grid.best_score_)

# 6. 회귀 계수 출력
coef_df_top = pd.DataFrame({
    'Feature': top_features,
    'Coefficient': grid.best_estimator_.coef_}).sort_values(by='Coefficient', key=abs, ascending=False)

print(coef_df_top)




# 모델 비교
# model 1 알파값
from statsmodels.formula.api import ols
model1 = ols(
    'SalePrice ~ OverallQual + ExterQual + KitchenQual + YearBuilt + BsmtFullBath + GrLivArea + TotalBsmtSF + GarageArea + WoodDeckSF + RoofStyle_Hip',
    data=df
).fit()

model1.summary()


# model 2 절대값
model2 = ols(
    formula='SalePrice ~ GrLivArea + OverallQual + TotalBsmtSF + GarageArea + YearBuilt + OverallCond + BsmtFullBath + ExterQual + KitchenQual + BsmtQual',
    data=df
).fit()

model2.summary()


import numpy as np
import pandas as pd
from sklearn.linear_model import Lasso
from sklearn.model_selection import GridSearchCV, KFold
from sklearn.preprocessing import StandardScaler
import statsmodels.api as sm




df

# 최종 선택 10개 변수
top_features 
df[top_features]



import matplotlib.pyplot as plt
import seaborn as sns



# 시각화
plt.figure(figsize=(16, 12))
for i, col in enumerate(top_features):
    plt.subplot(4, 3, i + 1)
    sns.scatterplot(data=df, x=col, y='SalePrice', s=20)
    plt.xlabel(col)
    plt.ylabel('')

plt.tight_layout()
plt.show()

# 이상치 조건
condition = (
    (df['GrLivArea'] <= 3500) &
    (df['GarageArea'] <= 1200) &
    (df['TotalBsmtSF'] <= 2500)
)

# 이상치 제거
df_clean = df[condition].copy()

import pandas as pd
from sklearn.preprocessing import MinMaxScaler


# 스케일링할 변수 목록
scale_columns = [
    'GrLivArea', 'TotalBsmtSF', 'GarageArea',   # 면적
    'OverallQual', 'KitchenQual', 'BsmtQual', 'ExterQual',  # 품질
    'OverallCond', 'BsmtFullBath',  # 상태
    'YearBuilt'  # 연식
]

# MinMaxScaler 적용 (0~1 범위)
scaler = MinMaxScaler()
df_clean[scale_columns] = scaler.fit_transform(df_clean[scale_columns])
df_clean = df_clean[scale_columns]


# 면적 그대로
area_score = df_clean['GrLivArea'] + df_clean['TotalBsmtSF'] + df_clean['GarageArea']

# 품질/상태/연식 뒤집기 (1 - 값)
quality_score = (1 - df_clean['OverallQual']) + (1 - df_clean['KitchenQual']) + (1 - df_clean['BsmtQual']) + (1 - df_clean['ExterQual'])
condition_score = (1 - df_clean['OverallCond']) + (1 - df_clean['BsmtFullBath'])
year_score = 1 - df_clean['YearBuilt']  # 연식도 반전

# 가중치 곱하고 합산
df['MaintenanceScore'] = (
    area_score * 0.2 +
    quality_score * 0.3 +
    condition_score * 0.2 +
    year_score * 0.3
)




df['Latitude']










# import statsmodels.api as sm

# def forward_stepwise(X, y, threshold_in=0.05, verbose=True):
#     initial_features = []
#     remaining_features = list(X.columns)
#     selected_features = []

#     while remaining_features:
#         aic_with_candidates = []
#         for candidate in remaining_features:
#             try:
#                 model = sm.OLS(y, sm.add_constant(X[initial_features + [candidate]])).fit()
#                 aic_with_candidates.append((model.aic, candidate))
#             except:
#                 continue
        
#         if not aic_with_candidates:
#             if verbose:
#                 print("⚠️ 남은 변수로 더 이상 개선 불가. 종료합니다.")
#             break

#         aic_with_candidates.sort()
#         best_aic, best_candidate = aic_with_candidates[0]

#         if verbose:
#             print(f"Try adding {best_candidate:>20} | AIC: {best_aic:.2f}")

#         initial_features.append(best_candidate)
#         remaining_features.remove(best_candidate)
#         selected_features = initial_features[:]

#     return selected_features




# X = df.drop(columns=['SalePrice'])
# y = df['SalePrice']

# selected = forward_stepwise(X, y)
# print("\n✅ 최종 선택된 변수 목록:")
# print(selected)
